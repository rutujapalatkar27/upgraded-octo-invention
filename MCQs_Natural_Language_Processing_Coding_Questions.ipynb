{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rutujapalatkar27/upgraded-octo-invention/blob/main/MCQs_Natural_Language_Processing_Coding_Questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "gKCQL1ACcixw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f209955e-5910-48db-ff18-628b9ab7c7ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from collections import defaultdict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 11"
      ],
      "metadata": {
        "id": "w1krxEJbdYj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example dictionary of words with their vectors\n",
        "word_vectors = {\n",
        "    'apple': np.array([[0.1, 0.2, 0.3]]),\n",
        "    'banana': np.array([[0.2, 0.1, 0.4]]),\n",
        "    'cherry': np.array([[0.3, 0.4, 0.2]]),\n",
        "    'date': np.array([[0.1, 0.5, 0.2]]),\n",
        "    'king': np.array([[0.4, 0.2, -0.3]])\n",
        "}\n",
        "\n",
        "def manhattan_distance(vec1, vec2):\n",
        "    \"\"\"\n",
        "    Calculate the Manhattan distance between two vectors.\n",
        "\n",
        "    Parameters:\n",
        "    vec1 (numpy array): First vector.\n",
        "    vec2 (numpy array): Second vector.\n",
        "\n",
        "    Returns:\n",
        "    float: Manhattan distance between vec1 and vec2.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate Manhattan distance\n",
        "    distance = np.sum(np.abs(vec1 - vec2))\n",
        "    return distance\n",
        "\n",
        "def find_most_similar_word(target_word, word_vectors):\n",
        "    # Check if the target word is in the dictionary\n",
        "    if target_word not in word_vectors:\n",
        "        raise ValueError(f\"The word '{target_word}' is not in the dictionary.\")\n",
        "\n",
        "    # Get the vector for the target word\n",
        "    target_vector = word_vectors[target_word]\n",
        "\n",
        "    # Compute manhattan distance\n",
        "    similarities = {}\n",
        "    for word, vector in word_vectors.items():\n",
        "        if word != target_word:\n",
        "            # Compute manhattan distance\n",
        "            dist = manhattan_distance(target_vector, vector)\n",
        "            similarities[word] = dist\n",
        "\n",
        "    # Find the most similar word\n",
        "    most_similar_word = min(similarities, key=similarities.get)\n",
        "\n",
        "    return most_similar_word\n",
        "\n",
        "# Example usage\n",
        "target_word = 'apple'\n",
        "most_similar = find_most_similar_word(target_word, word_vectors)\n",
        "print(f\"The most similar word to '{target_word}' is '{most_similar}'.\")"
      ],
      "metadata": {
        "id": "zXRkp0wzgYCg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25f808cd-4e1f-4475-9bb5-5210c35fac82"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most similar word to 'apple' is 'banana'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 12"
      ],
      "metadata": {
        "id": "NwFO9Th6imgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for stemming a single word\n",
        "def stem_word(word):\n",
        "    \"\"\"\n",
        "    Stems a single word using the Porter Stemmer algorithm.\n",
        "\n",
        "    Parameters:\n",
        "    - word (str): The word to be stemmed.\n",
        "\n",
        "    Returns:\n",
        "    - str: The stemmed word.\n",
        "    \"\"\"\n",
        "    # Initialize the Porter Stemmer\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Apply stemming to the word\n",
        "    return stemmer.stem(word)\n",
        "\n",
        "def convert_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Converts a sentence by applying stemming to each word.\n",
        "\n",
        "    Parameters:\n",
        "    - sentence (str): The sentence to be processed.\n",
        "\n",
        "    Returns:\n",
        "    - str: The sentence with each word stemmed.\n",
        "    \"\"\"\n",
        "    # Split the sentence into words and apply stemming to each word\n",
        "    output_sentence = sentence.split()\n",
        "    output_sentence = [stem_word(word) for word in output_sentence]\n",
        "\n",
        "    # Join the stemmed words back into a sentence\n",
        "    return \" \".join(output_sentence)\n",
        "\n",
        "# Example usage\n",
        "sentence = \"the children are playing in the garden and enjoying their time.\"\n",
        "# Convert the sentence by stemming each word\n",
        "output_sentence = convert_sentence(sentence)\n",
        "print(f\"Output Sentence: {output_sentence}\")"
      ],
      "metadata": {
        "id": "fkFu831hin75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7d72885-6667-48de-dddd-43ab868311da"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output Sentence: the children are play in the garden and enjoy their time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 13"
      ],
      "metadata": {
        "id": "55igYj-0xjgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(confusion_matrix):\n",
        "    \"\"\"\n",
        "    Calculates precision, recall, and F1-score from a confusion matrix.\n",
        "\n",
        "    Parameters:\n",
        "    - confusion_matrix (list of lists): The confusion matrix in the form [[TN, FP], [FN, TP]]\n",
        "\n",
        "    Returns:\n",
        "    - tuple: (precision, recall, f1-score)\n",
        "    \"\"\"\n",
        "    # Extract values from the confusion matrix\n",
        "    tn, fp = confusion_matrix[0]\n",
        "    fn, tp = confusion_matrix[1]\n",
        "\n",
        "    # Calculate precision, recall, and F1-score\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Example confusion matrix: [[TN, FP], [FN, TP]]\n",
        "confusion_matrix = [[50, 10], [5, 35]]\n",
        "\n",
        "# Calculate the metrics\n",
        "precision, recall, f1 = calculate_metrics(confusion_matrix)\n",
        "\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-score: {f1:.2f}\")"
      ],
      "metadata": {
        "id": "OWvU-Z0Bxiuu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3f7b48e-a2a9-4ae6-ac58-cbfc357b1945"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.78\n",
            "Recall: 0.88\n",
            "F1-score: 0.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 14"
      ],
      "metadata": {
        "id": "ZGiQNDUUAEYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess the text by converting it to lowercase,\n",
        "    removing punctuation, and tokenizing it into words.\n",
        "\n",
        "    Parameters:\n",
        "    - text (str): The input text to be preprocessed.\n",
        "\n",
        "    Returns:\n",
        "    - list: A list of words (tokens) from the text.\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    return words\n",
        "\n",
        "def build_vocabulary(corpus):\n",
        "    \"\"\"\n",
        "    Build a vocabulary of unique words from the corpus.\n",
        "\n",
        "    Parameters:\n",
        "    - corpus (list of str): The list of documents (texts).\n",
        "\n",
        "    Returns:\n",
        "    - list: A list of unique words (vocabulary).\n",
        "    \"\"\"\n",
        "    # Flatten the list of all words in the corpus\n",
        "    all_words = [word for doc in corpus for word in preprocess_text(doc)]\n",
        "\n",
        "    # Get unique words\n",
        "    vocabulary = list(set(all_words))\n",
        "\n",
        "    return vocabulary\n",
        "\n",
        "def vectorize_text(text, vocabulary):\n",
        "    \"\"\"\n",
        "    Create a Bag of Words vector for a given text.\n",
        "\n",
        "    Parameters:\n",
        "    - text (str): The input text to be vectorized.\n",
        "    - vocabulary (list of str): The list of unique words (vocabulary).\n",
        "\n",
        "    Returns:\n",
        "    - list: A list of integers representing the BoW vector.\n",
        "    \"\"\"\n",
        "    # Preprocess the text\n",
        "    words = preprocess_text(text)\n",
        "\n",
        "    # Create a Counter object to count word occurrences\n",
        "    word_counts = Counter(words)\n",
        "\n",
        "    # Create the vector based on the vocabulary\n",
        "    vector = [word_counts[word] for word in vocabulary]\n",
        "\n",
        "    return vector\n",
        "\n",
        "# Example corpus of documents\n",
        "corpus = [\n",
        "    \"The cat sat on the mat.\",\n",
        "    \"The dog chased the cat.\",\n",
        "    \"The cat climbed the tree.\",\n",
        "    \"The bat hanging on the tree.\",\n",
        "    \"The apple was kept on the table where the cat was sitting\"\n",
        "]\n",
        "\n",
        "# Build the vocabulary from the corpus\n",
        "vocabulary = build_vocabulary(corpus)\n",
        "\n",
        "text = \"The cat climbed on the where the bat was hanging.\"\n",
        "\n",
        "# Vectorize each document in the corpus\n",
        "bow_vector = vectorize_text(text, vocabulary)\n",
        "\n",
        "# Print the Bag of Words vectors\n",
        "print(f\"Bag of words vector {bow_vector}\")\n"
      ],
      "metadata": {
        "id": "ddhp3VHfAHRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fcd978b-45f4-4de5-ffb8-a81448e9b5a3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of words vector [1, 1, 0, 0, 1, 3, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 15\n",
        "\n",
        "\n",
        "### What is a Bigram?\n",
        "\n",
        "A **bigram** is a pair of consecutive words in a sequence, often used in natural language processing (NLP) to analyze the relationship between words in a sentence. It helps in understanding the likelihood of a word following another word, which is useful in tasks like text prediction, speech recognition, and more.\n",
        "\n",
        "### Example of a Bigram:\n",
        "\n",
        "Consider the sentence: \"I love pizza.\"\n",
        "\n",
        "- The bigrams in this sentence are:\n",
        "  - \"I love\"\n",
        "  - \"love pizza\"\n",
        "\n",
        "Each bigram represents a sequence of two consecutive words. If you have a large dataset of text, you can count how often each bigram appears. This helps you determine the probability of one word following another.\n",
        "\n",
        "### Bigram Probability:\n",
        "\n",
        "The **bigram probability** is the likelihood of the second word in the bigram appearing after the first word. For example, the probability of \"pizza\" appearing after \"love\" can be calculated using the formula:\n",
        "\n",
        "$$\n",
        "P(\\text{next_word} \\mid \\text{word}) = \\frac{\\text{Count of bigram (word, next_word)}}{\\text{Count of word}}\n",
        "$$\n",
        "\n",
        "\n",
        "This tells us how often \"pizza\" follows \"love\" compared to how often \"love\" appears overall.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b-Z-U6Ug0JR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramProbability:\n",
        "    def __init__(self):\n",
        "        self.bigram_counts = defaultdict(lambda: defaultdict(int))\n",
        "        self.unigram_counts = defaultdict(int)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        # Simple tokenization by splitting on non-alphabetical characters\n",
        "        return text.lower().split()\n",
        "\n",
        "    def train(self, sentences):\n",
        "        # Calculate bigram and unigram counts\n",
        "        for sentence in sentences:\n",
        "            tokens = self.tokenize(sentence)\n",
        "            for i in range(len(tokens) - 1):\n",
        "                word = tokens[i]\n",
        "                next_word = tokens[i + 1]\n",
        "                self.unigram_counts[word] += 1\n",
        "                self.bigram_counts[word][next_word] += 1\n",
        "\n",
        "    def probability(self, word, next_word):\n",
        "        # Calculate the conditional probability P(next_word | word)\n",
        "        if self.unigram_counts[word] == 0:\n",
        "            return 0.0\n",
        "        return self.bigram_counts[word][next_word] / self.unigram_counts[word]\n",
        "\n",
        "# Sample data\n",
        "sentences = [\n",
        "    \"I love this movie, it's fantastic!\",\n",
        "    \"This movie is horrible, I hate it.\",\n",
        "    \"What a wonderful movie, amazing experience!\",\n",
        "    \"The movie was the worst I've ever seen.\",\n",
        "    \"I really enjoyed the movie, it was great.\",\n",
        "    \"This was a terrible movie, never watching again.\"\n",
        "]\n",
        "\n",
        "# Create and train the BigramProbability model\n",
        "bigram_model = BigramProbability()\n",
        "bigram_model.train(sentences)\n",
        "\n",
        "# Example: Find the probability of the word \"movie\" being followed by \"is\"\n",
        "word = \"movie\"\n",
        "next_word = \"is\"\n",
        "prob = bigram_model.probability(word, next_word)\n",
        "print(f\"Probability of '{next_word}' occurring after '{word}': {prob:.2f}\")"
      ],
      "metadata": {
        "id": "tyOQfq430LWl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6da6d3cf-a6e9-4c90-879d-57f3671e5997"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability of 'is' occurring after 'movie': 0.50\n"
          ]
        }
      ]
    }
  ]
}