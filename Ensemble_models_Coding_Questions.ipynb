{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rutujapalatkar27/upgraded-octo-invention/blob/main/Ensemble_models_Coding_Questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import resample\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from scipy.stats import mode\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "metadata": {
        "id": "B3WLU4PbcrkI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 11"
      ],
      "metadata": {
        "id": "pWUONp6wcM4k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFRazT_-b92n"
      },
      "outputs": [],
      "source": [
        "def train_xgboost_classification(seed=42):\n",
        "    \"\"\"\n",
        "    This function generates a sample dataset using `make_classification`,\n",
        "    trains an XGBoost binary classifier, and returns the model's accuracy.\n",
        "\n",
        "    Parameters:\n",
        "    seed (int): Random seed for reproducibility. Default is 42.\n",
        "\n",
        "    Returns:\n",
        "    float: Accuracy of the XGBoost model on the test set.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Create a sample dataset with a fixed random seed for reproducibility\n",
        "    X, y = make_classification(n_samples=1000, n_features=10, n_classes=2,\n",
        "                               n_informative=5, n_redundant=2, random_state=seed)\n",
        "\n",
        "    # Convert the features and target to a DataFrame for easier manipulation\n",
        "    df = # write your code here\n",
        "    df['target'] = # write your code here\n",
        "\n",
        "    # Step 2: Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = # write your code here\n",
        "\n",
        "    # Step 3: Initialize and train an XGBoost model\n",
        "    # `use_label_encoder=False` disables the warning in recent XGBoost versions\n",
        "    # `eval_metric='logloss'` is used to specify the loss function for binary classification\n",
        "    xgb_model = # write your code here\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "\n",
        "    # Step 4: Make predictions on the test set\n",
        "    y_pred = # write your code here\n",
        "\n",
        "    # Step 5: Calculate and return the accuracy score\n",
        "    accuracy = # write your code here\n",
        "    return accuracy\n",
        "\n",
        "# Example usage:\n",
        "accuracy = train_xgboost_classification()\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 12"
      ],
      "metadata": {
        "id": "l4ZqlCs2Knyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ensemble_majority_voting(models, X_train, y_train, X_test_single):\n",
        "    \"\"\"\n",
        "    Train multiple models and perform classification using majority voting for a single input.\n",
        "\n",
        "    Parameters:\n",
        "    models (list): A list of initialized classification models.\n",
        "    X_train (array-like): Training data (features).\n",
        "    y_train (array-like): Training data (target labels).\n",
        "    X_test_single (array-like): A single test data point (features).\n",
        "\n",
        "    Returns:\n",
        "    int: Final prediction for the single input based on majority voting.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Train all models\n",
        "    for model in models:\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "    # Step 2: Generate predictions from each model for the single input data point\n",
        "    predictions = np.array([model.predict([X_test_single]) for model in models])\n",
        "\n",
        "    # Step 3: Perform majority voting (mode of predictions)\n",
        "    final_prediction = mode(predictions, axis=0)[0].flatten()[0]\n",
        "\n",
        "    return final_prediction\n",
        "\n",
        "\n",
        "# Example usage of the function\n",
        "\n",
        "# Step 1: Create a sample dataset\n",
        "X, y = make_classification(n_samples=100, n_features=4, n_informative=2,\n",
        "                           n_redundant=0, random_state=42)\n",
        "\n",
        "# Step 2: Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = # write your code here\n",
        "\n",
        "#Hyperparameters for model training\n",
        "n_estimators = 10\n",
        "random_state=42\n",
        "kernel = 'linear'\n",
        "\n",
        "# Step 3: Define 4 different classifiers\n",
        "model1 = # write your code here\n",
        "model2 = # write your code here\n",
        "model3 = # write your code here\n",
        "model4 = # write your code here\n",
        "\n",
        "# Step 4: Use the ensemble_majority_voting function for a single input\n",
        "models = [model1, model2, model3, model4]\n",
        "X_test_single = X_test[0]  # Using only one data point from the test set\n",
        "\n",
        "final_prediction = # write your code here\n",
        "\n",
        "# Display the final combined prediction for the single input\n",
        "print(\"Final Combined Prediction (Majority Voting) for single input:\", final_prediction)"
      ],
      "metadata": {
        "id": "MncqKQbWM2XE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 13"
      ],
      "metadata": {
        "id": "j4sEanytQ41I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_classifiers(seed=42):\n",
        "    \"\"\"\n",
        "    This function generates a sample dataset, trains Decision Tree, and Random Forest\n",
        "    classifiers, and compares their accuracy.\n",
        "\n",
        "    Parameters:\n",
        "    seed (int): Random seed for reproducibility. Default is 42.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary containing accuracy scores for each classifier.\n",
        "    \"\"\"\n",
        "\n",
        "    # Suppress warnings\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "    # Step 1: Create a sample dataset with a fixed random seed for reproducibility\n",
        "    X, y = make_classification(n_samples=1000, n_features=10, n_classes=2,\n",
        "                               n_informative=5, n_redundant=2, random_state=seed)\n",
        "\n",
        "    # Convert the features and target to a DataFrame for easier manipulation\n",
        "    df = # write your code here\n",
        "    df['target'] = # write your code here\n",
        "\n",
        "    # Step 2: Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = # write your code here\n",
        "\n",
        "    # Dictionary to store accuracy scores\n",
        "    accuracy_scores = {}\n",
        "\n",
        "    # Step 3: Train Decision Tree model\n",
        "    dt_model = # write your code here\n",
        "    dt_model.fit(X_train, y_train)\n",
        "    y_pred_dt = # write your code here\n",
        "    accuracy_scores['Decision Tree'] = # write your code here\n",
        "\n",
        "    # Step 4: Train Random Forest model\n",
        "    rf_model = # write your code here\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    y_pred_rf = # write your code here\n",
        "    accuracy_scores['Random Forest'] = # write your code here\n",
        "\n",
        "    return accuracy_scores\n",
        "\n",
        "# Example usage:\n",
        "accuracy_results = compare_classifiers()\n",
        "for model, accuracy in accuracy_results.items():\n",
        "    print(f\"{model} Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "7MZORe2TZhEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 14"
      ],
      "metadata": {
        "id": "imLfT_C0Qwom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_ensemble_voting(models, weights, X_train, y_train, X_test_single):\n",
        "    \"\"\"\n",
        "    Train multiple models and perform weighted voting for a single input.\n",
        "\n",
        "    Parameters:\n",
        "    models (list): A list of initialized classification models.\n",
        "    weights (list): A list of weights corresponding to each model.\n",
        "    X_train (array-like): Training data (features).\n",
        "    y_train (array-like): Training data (target labels).\n",
        "    X_test_single (array-like): A single test data point (features).\n",
        "\n",
        "    Returns:\n",
        "    int: Final prediction for the single input based on weighted voting.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Train all models\n",
        "    for model in models:\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "    # Step 2: Generate weighted predictions from each model for the single input\n",
        "    predictions = np.array([model.predict([X_test_single])[0] for model in models])\n",
        "\n",
        "    # Step 3: Calculate weighted sum of predictions for each class (0 and 1 in binary classification)\n",
        "    weighted_votes = {}\n",
        "    unique_classes = np.unique(y_train)\n",
        "\n",
        "    for cls in unique_classes:\n",
        "        weighted_votes[cls] = # write your code here\n",
        "\n",
        "    # Step 4: Return the class with the highest weighted vote\n",
        "    final_prediction = # write your code here\n",
        "\n",
        "    return final_prediction\n",
        "\n",
        "# Example usage of the function\n",
        "\n",
        "# Step 1: Create a sample dataset\n",
        "X, y = make_classification(n_samples=100, n_features=4, n_informative=2,\n",
        "                           n_redundant=0, random_state=42)\n",
        "\n",
        "# Step 2: Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
        "                                                    random_state=42)\n",
        "\n",
        "#Hyperparameters for model training\n",
        "n_estimators = 10\n",
        "random_state=42\n",
        "kernel = 'linear'\n",
        "\n",
        "# Step 3: Define 4 different classifiers\n",
        "model1 = # write your code here\n",
        "model2 = # write your code here\n",
        "model3 = # write your code here\n",
        "model4 = # write your code here\n",
        "\n",
        "# Step 4: Define the weights for each model (higher weight for models we trust more)\n",
        "weights = [0.4, 0.3, 0.2, 0.1]  # Total should sum to 1, but the ratio matters more than the absolute value\n",
        "\n",
        "# Step 5: Use the weighted_ensemble_voting function for a single input\n",
        "models = [model1, model2, model3, model4]\n",
        "X_test_single = X_test[0]  # Using only one data point from the test set\n",
        "\n",
        "final_prediction = weighted_ensemble_voting(models, weights, X_train, y_train,\n",
        "                                            X_test_single)\n",
        "\n",
        "# Display the final combined prediction for the single input\n",
        "print(\"Class:\", final_prediction)"
      ],
      "metadata": {
        "id": "xEQtXsSjQw7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 15"
      ],
      "metadata": {
        "id": "3jsjXoSch0pa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gradient_boosting(seed=42):\n",
        "    \"\"\"\n",
        "    This function generates a sample dataset using `make_classification`,\n",
        "    trains a Gradient Boosting classifier, and returns the model's accuracy.\n",
        "\n",
        "    Parameters:\n",
        "    seed (int): Random seed for reproducibility. Default is 42.\n",
        "\n",
        "    Returns:\n",
        "    float: Accuracy of the Gradient Boosting model on the test set.\n",
        "    \"\"\"\n",
        "\n",
        "    # Suppress warnings\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "    # Step 1: Create a sample dataset with a fixed random seed for reproducibility\n",
        "    X, y = make_classification(n_samples=1000, n_features=10, n_classes=2,\n",
        "                               n_informative=5, n_redundant=2, random_state=seed)\n",
        "\n",
        "    # Convert the features and target to a DataFrame for easier manipulation\n",
        "    df = # write your code here\n",
        "    df['target'] = # write your code here\n",
        "\n",
        "    # Step 2: Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = # write your code here\n",
        "\n",
        "    # Step 3: Train Gradient Boosting model\n",
        "    gb_model = # write your code here\n",
        "    gb_model.fit(X_train, y_train)\n",
        "\n",
        "    # Step 4: Make predictions on the test set\n",
        "    y_pred_gb = # write your code here\n",
        "\n",
        "    # Step 5: Calculate and return the accuracy score\n",
        "    accuracy = # write your code here\n",
        "    return accuracy\n",
        "\n",
        "# Example usage:\n",
        "accuracy = train_gradient_boosting()\n",
        "print(f\"Gradient Boosting Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "zDOICnVUZnpy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}